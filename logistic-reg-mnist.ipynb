{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Starter code for logistic regression model to solve OCR task \n",
    "with MNIST in TensorFlow\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "Author: Chip Huyen\n",
    "Prepared for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "cs20si.stanford.edu\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "beta = 0.001\n",
    "batch_size = 256\n",
    "n_epochs = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets('/data/mnist', one_hot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# Features are of the type float, and labels are of the type int\n",
    "X = tf.placeholder(tf.float32, name=\"Features\", shape=(None, 784))\n",
    "Y = tf.placeholder(tf.int32, name=\"Labels\", shape=(None, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: create weights and bias\n",
    "# weights and biases are initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = X * w + b\n",
    "# shape of b depends on Y\n",
    "#w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "#b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "W1 = tf.get_variable(\"W1\", [784,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "b1 = tf.get_variable(\"b1\", [1,64], initializer = tf.zeros_initializer())\n",
    "W2 = tf.get_variable(\"W2\", [64,32], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "b2 = tf.get_variable(\"b2\", [1,32], initializer = tf.zeros_initializer())\n",
    "W3 = tf.get_variable(\"W3\", [32,10], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "b3 = tf.get_variable(\"b3\", [1,10], initializer = tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "# to get the probability distribution of possible label of the image\n",
    "# DO NOT DO SOFTMAX HERE\n",
    "#Z = tf.add(tf.matmul(X,w),b)\n",
    "Z1 = tf.add(tf.matmul(X, W1), b1)                                             \n",
    "A1 = tf.nn.relu(Z1)                                             \n",
    "Z2 = tf.add(tf.matmul(A1, W2), b2)                                             \n",
    "A2 = tf.nn.relu(Z2)                                            \n",
    "Z3 = tf.add(tf.matmul(A2, W3), b3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 5: define loss function\n",
    "# use cross entropy loss of the real labels with the softmax of logits\n",
    "# use the method:\n",
    "# tf.nn.softmax_cross_entropy_with_logits(logits, Y)\n",
    "# then use tf.reduce_mean to get the mean loss of the batch\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))\n",
    "regularizer1 = tf.nn.l2_loss(W1)\n",
    "regularizer2 = tf.nn.l2_loss(W2)\n",
    "regularizer3 = tf.nn.l2_loss(W3)\n",
    "loss = tf.reduce_mean(loss + beta*(regularizer1 + regularizer2 + regularizer3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6: define training op\n",
    "# using gradient descent to minimize loss\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "starter_learning_rate = 0.1\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 32, 0.95, staircase=True)\n",
    "optimizer = optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss,global_step=global_step)\n",
    "\n",
    "#optimizer = optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 0.7728748001227869\n",
      "Average loss epoch 1: 0.411639711149385\n",
      "Average loss epoch 2: 0.3568742993836091\n",
      "Average loss epoch 3: 0.32168084160189764\n",
      "Average loss epoch 4: 0.2978843773636862\n",
      "Average loss epoch 5: 0.2800571486075348\n",
      "Average loss epoch 6: 0.2658345499467627\n",
      "Average loss epoch 7: 0.255087661478564\n",
      "Average loss epoch 8: 0.24209190229667682\n",
      "Average loss epoch 9: 0.23413316153477284\n",
      "Average loss epoch 10: 0.22536528730225341\n",
      "Average loss epoch 11: 0.21862419165461977\n",
      "Average loss epoch 12: 0.21392637017731356\n",
      "Average loss epoch 13: 0.20693385176290977\n",
      "Average loss epoch 14: 0.20215969205459702\n",
      "Average loss epoch 15: 0.19807841113516103\n",
      "Average loss epoch 16: 0.19419594060316264\n",
      "Average loss epoch 17: 0.18858105967813563\n",
      "Average loss epoch 18: 0.18635076249592772\n",
      "Average loss epoch 19: 0.18374015223757129\n",
      "Average loss epoch 20: 0.17886569065468333\n",
      "Average loss epoch 21: 0.17519533564554196\n",
      "Average loss epoch 22: 0.17491854504447116\n",
      "Average loss epoch 23: 0.170637568928928\n",
      "Average loss epoch 24: 0.1684732892872574\n",
      "Average loss epoch 25: 0.16605208013380798\n",
      "Average loss epoch 26: 0.1645506723113706\n",
      "Average loss epoch 27: 0.16231446027337948\n",
      "Average loss epoch 28: 0.15990166091055513\n",
      "Average loss epoch 29: 0.1588757645652116\n",
      "Average loss epoch 30: 0.15754770215983702\n",
      "Average loss epoch 31: 0.15398509866584129\n",
      "Average loss epoch 32: 0.15372479964639538\n",
      "Average loss epoch 33: 0.15234434117223614\n",
      "Average loss epoch 34: 0.15148641478931793\n",
      "Average loss epoch 35: 0.14869228364011952\n",
      "Average loss epoch 36: 0.1487862975728289\n",
      "Average loss epoch 37: 0.1462217473607754\n",
      "Average loss epoch 38: 0.1461760537507378\n",
      "Average loss epoch 39: 0.14556838746104284\n",
      "Average loss epoch 40: 0.14299020024939119\n",
      "Average loss epoch 41: 0.1435248605628437\n",
      "Average loss epoch 42: 0.14168940700381716\n",
      "Average loss epoch 43: 0.14141364193686815\n",
      "Average loss epoch 44: 0.14023992224274395\n",
      "Average loss epoch 45: 0.13939662033987937\n",
      "Average loss epoch 46: 0.1380787946144554\n",
      "Average loss epoch 47: 0.13870183117339543\n",
      "Average loss epoch 48: 0.13793426497935135\n",
      "Average loss epoch 49: 0.1360333846342341\n",
      "Average loss epoch 50: 0.13541863809957683\n",
      "Average loss epoch 51: 0.13591808745654943\n",
      "Average loss epoch 52: 0.13485495533759348\n",
      "Average loss epoch 53: 0.13521620317875782\n",
      "Average loss epoch 54: 0.133400129763601\n",
      "Average loss epoch 55: 0.13163582113719433\n",
      "Average loss epoch 56: 0.1319622774826032\n",
      "Average loss epoch 57: 0.13234053691413916\n",
      "Average loss epoch 58: 0.13092147914048669\n",
      "Average loss epoch 59: 0.13147622573180734\n",
      "Average loss epoch 60: 0.1302147478422272\n",
      "Average loss epoch 61: 0.1307343359096585\n",
      "Average loss epoch 62: 0.12929342318082523\n",
      "Average loss epoch 63: 0.1297465843783918\n",
      "Average loss epoch 64: 0.1293582973009515\n",
      "Average loss epoch 65: 0.12827601085338636\n",
      "Average loss epoch 66: 0.1277146916662421\n",
      "Average loss epoch 67: 0.12864768160420043\n",
      "Average loss epoch 68: 0.12733379709664908\n",
      "Average loss epoch 69: 0.12771326891869028\n",
      "Average loss epoch 70: 0.12608728077367087\n",
      "Average loss epoch 71: 0.12701671525278938\n",
      "Average loss epoch 72: 0.12589521831441147\n",
      "Average loss epoch 73: 0.12631496115126342\n",
      "Average loss epoch 74: 0.12562030508557212\n",
      "Average loss epoch 75: 0.1250331874275319\n",
      "Average loss epoch 76: 0.12514961841646757\n",
      "Average loss epoch 77: 0.1244859378610816\n",
      "Average loss epoch 78: 0.12552860243437447\n",
      "Average loss epoch 79: 0.12429660363732098\n",
      "Average loss epoch 80: 0.12415781940951526\n",
      "Average loss epoch 81: 0.12487050682027763\n",
      "Average loss epoch 82: 0.1234126128256321\n",
      "Average loss epoch 83: 0.12366380284879809\n",
      "Average loss epoch 84: 0.12338571532446647\n",
      "Average loss epoch 85: 0.12334702985587521\n",
      "Average loss epoch 86: 0.1223651575290154\n",
      "Average loss epoch 87: 0.12253650653863622\n",
      "Average loss epoch 88: 0.12354610647971385\n",
      "Average loss epoch 89: 0.12228138918910071\n",
      "Average loss epoch 90: 0.1212114012478111\n",
      "Average loss epoch 91: 0.12294182999529571\n",
      "Average loss epoch 92: 0.12225310957041856\n",
      "Average loss epoch 93: 0.12159627595933799\n",
      "Average loss epoch 94: 0.12162082482164151\n",
      "Average loss epoch 95: 0.12115028159362133\n",
      "Average loss epoch 96: 0.12119275644839367\n",
      "Average loss epoch 97: 0.1213791453253443\n",
      "Average loss epoch 98: 0.12152284167915861\n",
      "Average loss epoch 99: 0.12152818207428834\n",
      "Average loss epoch 100: 0.12083423604196478\n",
      "Average loss epoch 101: 0.12009582334311208\n",
      "Average loss epoch 102: 0.11978605953610946\n",
      "Average loss epoch 103: 0.12044835466647817\n",
      "Average loss epoch 104: 0.1207290228421443\n",
      "Average loss epoch 105: 0.12068129369170866\n",
      "Average loss epoch 106: 0.1200871887120688\n",
      "Average loss epoch 107: 0.12004054745084772\n",
      "Average loss epoch 108: 0.11934631561564508\n",
      "Average loss epoch 109: 0.12060374585546066\n",
      "Average loss epoch 110: 0.11871042422880636\n",
      "Average loss epoch 111: 0.1202005135052115\n",
      "Average loss epoch 112: 0.12017425595321388\n",
      "Average loss epoch 113: 0.11940254660967355\n",
      "Average loss epoch 114: 0.11954110397774483\n",
      "Average loss epoch 115: 0.11812798118340635\n",
      "Average loss epoch 116: 0.12019286308193876\n",
      "Average loss epoch 117: 0.11849766120175335\n",
      "Average loss epoch 118: 0.11879804253438923\n",
      "Average loss epoch 119: 0.11950178837804036\n",
      "Average loss epoch 120: 0.1182410803701833\n",
      "Average loss epoch 121: 0.11971593561155774\n",
      "Average loss epoch 122: 0.11755758137485692\n",
      "Average loss epoch 123: 0.11915396460305865\n",
      "Average loss epoch 124: 0.11871815786183437\n",
      "Average loss epoch 125: 0.11825095496584322\n",
      "Average loss epoch 126: 0.11842953305795928\n",
      "Average loss epoch 127: 0.11834304929475918\n",
      "Average loss epoch 128: 0.11807344892175398\n",
      "Average loss epoch 129: 0.11850451667592904\n",
      "Average loss epoch 130: 0.11767917975922611\n",
      "Average loss epoch 131: 0.1191369680531114\n",
      "Average loss epoch 132: 0.11680818512757248\n",
      "Average loss epoch 133: 0.11822228586283799\n",
      "Average loss epoch 134: 0.1180928782603451\n",
      "Average loss epoch 135: 0.11801756476269704\n",
      "Average loss epoch 136: 0.11778822482050022\n",
      "Average loss epoch 137: 0.11732277733700297\n",
      "Average loss epoch 138: 0.11759113966026039\n",
      "Average loss epoch 139: 0.11764359502034767\n",
      "Average loss epoch 140: 0.11707079971087313\n",
      "Average loss epoch 141: 0.11792036814388827\n",
      "Average loss epoch 142: 0.11710984534888624\n",
      "Average loss epoch 143: 0.1170447758425062\n",
      "Average loss epoch 144: 0.11810818596560264\n",
      "Average loss epoch 145: 0.11711102508217375\n",
      "Average loss epoch 146: 0.1172306893793779\n",
      "Average loss epoch 147: 0.1166534818987423\n",
      "Average loss epoch 148: 0.11807670698405426\n",
      "Average loss epoch 149: 0.11716104260532656\n",
      "Average loss epoch 150: 0.11605767522738358\n",
      "Average loss epoch 151: 0.11657259968396659\n",
      "Average loss epoch 152: 0.11716493727447831\n",
      "Average loss epoch 153: 0.11637126877207622\n",
      "Average loss epoch 154: 0.11702795096925486\n",
      "Average loss epoch 155: 0.11691497172289919\n",
      "Average loss epoch 156: 0.1168610723079922\n",
      "Average loss epoch 157: 0.11701455938621103\n",
      "Average loss epoch 158: 0.11701968963318896\n",
      "Average loss epoch 159: 0.11615397631425724\n",
      "Average loss epoch 160: 0.1166235623167497\n",
      "Average loss epoch 161: 0.1173076332659922\n",
      "Average loss epoch 162: 0.11580779918721903\n",
      "Average loss epoch 163: 0.11681862158056731\n",
      "Average loss epoch 164: 0.11662839464495116\n",
      "Average loss epoch 165: 0.11591064811588447\n",
      "Average loss epoch 166: 0.11664750931836734\n",
      "Average loss epoch 167: 0.11608949070361173\n",
      "Average loss epoch 168: 0.11688231760374854\n",
      "Average loss epoch 169: 0.11591363196896616\n",
      "Average loss epoch 170: 0.1162215165933159\n",
      "Average loss epoch 171: 0.11671669903063328\n",
      "Average loss epoch 172: 0.11520688225314996\n",
      "Average loss epoch 173: 0.11669997345203552\n",
      "Average loss epoch 174: 0.11558383128771157\n",
      "Average loss epoch 175: 0.11622317493936726\n",
      "Average loss epoch 176: 0.11570222957808281\n",
      "Average loss epoch 177: 0.1161191476004146\n",
      "Average loss epoch 178: 0.11613686515070568\n",
      "Average loss epoch 179: 0.11618875102879846\n",
      "Average loss epoch 180: 0.11497501454481454\n",
      "Average loss epoch 181: 0.11669983478071534\n",
      "Average loss epoch 182: 0.11516526278769859\n",
      "Average loss epoch 183: 0.11583971921528612\n",
      "Average loss epoch 184: 0.11583965209042915\n",
      "Average loss epoch 185: 0.11511787692102317\n",
      "Average loss epoch 186: 0.11632434560734535\n",
      "Average loss epoch 187: 0.1148025986399049\n",
      "Average loss epoch 188: 0.11607315095367833\n",
      "Average loss epoch 189: 0.11507978648922154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 190: 0.11588600653911306\n",
      "Average loss epoch 191: 0.11547177100432253\n",
      "Average loss epoch 192: 0.11545249962500323\n",
      "Average loss epoch 193: 0.11540096308861937\n",
      "Average loss epoch 194: 0.11607970241631303\n",
      "Average loss epoch 195: 0.11514277933774708\n",
      "Average loss epoch 196: 0.1159184051103124\n",
      "Average loss epoch 197: 0.1151922530729637\n",
      "Average loss epoch 198: 0.11572990099960398\n",
      "Average loss epoch 199: 0.1149165990684077\n",
      "Average loss epoch 200: 0.11513451253560102\n",
      "Average loss epoch 201: 0.11538451857795225\n",
      "Average loss epoch 202: 0.11517675917282283\n",
      "Average loss epoch 203: 0.11541998062596143\n",
      "Average loss epoch 204: 0.11505139236138245\n",
      "Average loss epoch 205: 0.11483372688711246\n",
      "Average loss epoch 206: 0.11494860860788933\n",
      "Average loss epoch 207: 0.11564397693515938\n",
      "Average loss epoch 208: 0.11501895329941099\n",
      "Average loss epoch 209: 0.11493429990711614\n",
      "Average loss epoch 210: 0.1147960953275177\n",
      "Average loss epoch 211: 0.11537655429026791\n",
      "Average loss epoch 212: 0.11533270295813819\n",
      "Average loss epoch 213: 0.11408009282617926\n",
      "Average loss epoch 214: 0.11519485774719826\n",
      "Average loss epoch 215: 0.11434785443767209\n",
      "Average loss epoch 216: 0.11499613544372754\n",
      "Average loss epoch 217: 0.11459276797336952\n",
      "Average loss epoch 218: 0.11508165460045093\n",
      "Average loss epoch 219: 0.11455881414569427\n",
      "Average loss epoch 220: 0.1146590898736058\n",
      "Average loss epoch 221: 0.11503348583094428\n",
      "Average loss epoch 222: 0.11451433181205642\n",
      "Average loss epoch 223: 0.11460447791859368\n",
      "Average loss epoch 224: 0.11484805957596993\n",
      "Average loss epoch 225: 0.11468839655830482\n",
      "Average loss epoch 226: 0.11492901190976117\n",
      "Average loss epoch 227: 0.11474741111013377\n",
      "Average loss epoch 228: 0.11417989693930215\n",
      "Average loss epoch 229: 0.11468121691424156\n",
      "Average loss epoch 230: 0.11422519312701493\n",
      "Average loss epoch 231: 0.11451027705986923\n",
      "Average loss epoch 232: 0.11404080974442937\n",
      "Average loss epoch 233: 0.11445934788506722\n",
      "Average loss epoch 234: 0.11448674046686877\n",
      "Average loss epoch 235: 0.114412695830949\n",
      "Average loss epoch 236: 0.11422455530161056\n",
      "Average loss epoch 237: 0.11462608373192983\n",
      "Average loss epoch 238: 0.11403538843738699\n",
      "Average loss epoch 239: 0.11463703685135485\n",
      "Average loss epoch 240: 0.11428482903637618\n",
      "Average loss epoch 241: 0.11416811224456146\n",
      "Average loss epoch 242: 0.1139666699917517\n",
      "Average loss epoch 243: 0.11425573462359259\n",
      "Average loss epoch 244: 0.11448215752422253\n",
      "Average loss epoch 245: 0.11414322124741902\n",
      "Average loss epoch 246: 0.11407230383603373\n",
      "Average loss epoch 247: 0.11435111612081528\n",
      "Average loss epoch 248: 0.11410012983969439\n",
      "Average loss epoch 249: 0.11424730345606804\n",
      "Average loss epoch 250: 0.11397343114993283\n",
      "Average loss epoch 251: 0.11424245344144161\n",
      "Average loss epoch 252: 0.11401440718463648\n",
      "Average loss epoch 253: 0.11402264731788189\n",
      "Average loss epoch 254: 0.11390060203794007\n",
      "Average loss epoch 255: 0.11400684279119858\n",
      "Total time: 169.16899991035461 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9772\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        total_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # TO-DO: run optimizer + fetch loss_batch\n",
    "            _,loss_batch = sess.run([optimizer,loss],feed_dict={X:X_batch,Y:Y_batch})\n",
    "            # \n",
    "            total_loss += loss_batch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    print('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "    # test the model\n",
    "    preds = tf.nn.softmax(Z3)\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run([accuracy], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        total_correct_preds += accuracy_batch[0]\n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del W1, b1,W2,b2,W3,b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
